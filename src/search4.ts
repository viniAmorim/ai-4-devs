// search3.ts
import { RedisVectorStore } from '@langchain/redis';
// Importa ambas as inst√¢ncias e o cliente Redis
import { jsonVectorStore, pdfVectorStore, redisClient } from './redis-store';
import { PromptTemplate } from '@langchain/core/prompts';
import { HuggingFaceInference } from '@langchain/community/llms/hf';
import dotenv from 'dotenv';
import readline from 'node:readline/promises';

dotenv.config();

// ... (LLM_PROMPT e resultsPrompt permanecem os mesmos) ...
const LLM_PROMPT = PromptTemplate.fromTemplate(`[INST]
Voc√™ √© um assistente de IA focado em fornecer respostas precisas, concisas e √∫teis com base EXCLUSIVAMENTE no contexto fornecido.
N√£o utilize conhecimento externo.
Se a informa√ß√£o necess√°ria para responder √† pergunta n√£o estiver explicitamente no contexto, responda clara e diretamente que "N√£o foi poss√≠vel encontrar informa√ß√µes relevantes para a sua consulta na nossa base de dados."

Contexto:
{context}

Pergunta: {query}

Com base no contexto fornecido, responda √† pergunta de forma concisa e coerente.
[/INST]`);

const resultsPrompt = PromptTemplate.fromTemplate(`
 ## An√°lise de Documentos

 ### Contexto da Busca:
 - Termo pesquisado: "{query}"
 - N√∫mero de resultados (brutos): {rawCount}
 - Modelo de Embedding utilizado: {embeddingModel}

 ### Resultados Encontrados (ap√≥s filtro de relev√¢ncia):
 {results}

 ### Detalhes da An√°lise (para o desenvolvedor/monitoramento):
 1. Identifique os 3 principais insights
 2. Destaque termos-chave relevantes
 3. Avalie a consist√™ncia entre os documentos
`);


async function semanticSearch(query: string, k: number = 3, searchType: 'json' | 'pdf' = 'json') { // Adiciona searchType
    let llmResponse = "";
    try {
        console.log(`\nüîç Iniciando busca por: "${query}" no √≠ndice de ${searchType.toUpperCase()}...`);

        if (!redisClient.isOpen) { // Usa o redisClient exportado
            await redisClient.connect();
        }

        let currentVectorStore: RedisVectorStore;
        if (searchType === 'pdf') {
            currentVectorStore = pdfVectorStore;
        } else {
            currentVectorStore = jsonVectorStore;
        }
        
        let results: [any, number][] = [];
        let embeddingModelUsed = process.env.EMBEDDINGS_MODEL || 'API Padr√£o';

        try {
            results = await currentVectorStore.similaritySearchWithScore(query, k);
        } catch (apiError) {
            console.warn('‚ö†Ô∏è Alternando para embeddings locais...');
            const { HuggingFaceTransformersEmbeddings } = await import('@langchain/community/embeddings/hf_transformers');
            embeddingModelUsed = 'Xenova/all-MiniLM-L6-v2 (local)';

            const localEmbeddings = new HuggingFaceTransformersEmbeddings({
                modelName: embeddingModelUsed
            });

            // Se for necess√°rio usar o RedisVectorStore local, precisa recri√°-lo com o cliente redis
            const localStore = new RedisVectorStore(localEmbeddings, {
                redisClient: redisClient, // Usa o redisClient exportado
                indexName: currentVectorStore.indexName // Mant√©m o mesmo indexName da store original
            });

            results = await localStore.similaritySearchWithScore(query, k);
        }

        const RELEVANCE_THRESHOLD = 0.4;
        const filteredResults = results.filter(([, score]) => score >= RELEVANCE_THRESHOLD);

        let formattedRawResultsForDebug = '';
        let contextForLLM = '';

        if (filteredResults.length === 0) {
            formattedRawResultsForDebug = 'Nenhum documento relevante encontrado ap√≥s filtragem.';
            llmResponse = "N√£o foi poss√≠vel encontrar informa√ß√µes relevantes para a sua consulta na nossa base de dados.";
        } else {
            formattedRawResultsForDebug = filteredResults.map(([doc, score], index) => (
                `\n### Documento ${index + 1} (Score: ${score.toFixed(3)})\n` +
                `**T√≠tulo:** ${doc.metadata?.titulo || 'Sem t√≠tulo'}\n` +
                `**Trecho:** ${doc.pageContent.substring(0, 300)}${doc.pageContent.length > 300 ? '...' : ''}`
            )).join('\n');

            contextForLLM = filteredResults.map(([doc, score], index) => (
                `Documento ${index + 1}:\n${doc.pageContent}`
            )).join('\n\n');

            const chatModelName = process.env.CHAT_MODEL;
            const huggingFaceApiKey = process.env.HUGGINGFACEHUB_API_TOKEN; 

            if (!chatModelName || !huggingFaceApiKey) {
                console.error("Vari√°veis de ambiente CHAT_MODEL ou HUGGINGFACEHUB_API_TOKEN n√£o configuradas. N√£o ser√° poss√≠vel gerar respostas.");
                llmResponse = "Erro: Modelo de chat ou chave API n√£o configurados para gerar respostas.";
            } else {
                const huggingFaceLLM = new HuggingFaceInference({
                    model: chatModelName,
                    apiKey: huggingFaceApiKey,
                    maxTokens: 300, 
                    temperature: 0.3, 
                    maxRetries: 5, // Tentar at√© 5 vezes em caso de falha tempor√°ria
                    timeout: 60000 // Aumenta o timeout para 60 segundos (o padr√£o √© 10 segundos), dando mais tempo para o modelo "acordar"
                });

                try {
                    const llmPromptFormatted = await LLM_PROMPT.format({
                        query: query,
                        context: contextForLLM
                    });

                    const generatedText = await huggingFaceLLM.invoke(llmPromptFormatted, {
                        maxTokens: 300, 
                        temperature: 0.3, 
                    });

                    const cleanGeneratedText = generatedText.split('[/INST]').pop()?.trim() || generatedText.trim();
                    llmResponse = cleanGeneratedText;

                } catch (llmError: any) {
                    console.error("‚ùå Erro ao chamar o LLM:", llmError.message || llmError);
                    llmResponse = "Houve um erro t√©cnico ao gerar a resposta. Por favor, tente novamente mais tarde.";
                }
            }
        }

        console.log('\n‚úÖ Resposta Gerada:');
        console.log(llmResponse);

        console.log('\n--- Painel de An√°lise (Detalhes Internos) ---');
        const analysisPrompt = await resultsPrompt.format({
            query,
            rawCount: results.length,
            embeddingModel: embeddingModelUsed,
            results: formattedRawResultsForDebug
        });
        console.log(analysisPrompt);

        console.log('\nüîç Detalhes T√©cnicos Finais:');
        console.log(`Modelo LLM: ${process.env.CHAT_MODEL || 'N√£o configurado'}`);
        console.log(`Modelo Embedding: ${embeddingModelUsed}`);
        console.log(`Tempo: ${new Date().toLocaleTimeString()}`);

    } finally {
        // A desconex√£o ser√° gerenciada no main() agora
    }
}

// Interface interativa melhorada
async function main() {
    const rl = readline.createInterface({
        input: process.stdin,
        output: process.stdout
    });

    try {
        console.log('\n=== Sistema de An√°lise Sem√¢ntica ===');
        console.log('üìå Modo:', process.env.HUGGINGFACEHUB_API_TOKEN ? 'API' : 'Local');

        // Conecta o cliente Redis uma vez no in√≠cio
        if (!redisClient.isOpen) {
            await redisClient.connect();
            console.log('üîå Conectado ao Redis.');
        }

        while (true) {
            const query = await rl.question('\nüîé Digite sua consulta (ou "sair"): ');
            if (query.toLowerCase() === 'sair') break;

            const countInput = await rl.question('üìä N√∫mero de resultados para busca (3): ') || '3';
            let searchTypeInput = await rl.question('üìö Buscar em (json/pdf - padr√£o json): ') || 'json';
            searchTypeInput = searchTypeInput.toLowerCase();

            if (searchTypeInput !== 'json' && searchTypeInput !== 'pdf') {
                console.warn('Tipo de busca inv√°lido. Usando "json" como padr√£o.');
                searchTypeInput = 'json';
            }

            await semanticSearch(query, parseInt(countInput), searchTypeInput as 'json' | 'pdf');
        }
    } finally {
        rl.close();
        if (redisClient.isOpen) { // Garante a desconex√£o no final da sess√£o
            console.log('\nDesconectando do Redis...');
            await redisClient.disconnect();
        }
        console.log('\nüõë Sess√£o encerrada');
    }
}

main().catch(err => {
    console.error('\n‚ùå Falha no sistema:', err);
    process.exit(1);
});