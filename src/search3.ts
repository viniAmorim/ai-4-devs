import { RedisVectorStore } from '@langchain/redis';
import { redisVectorStore } from './redis-store';
import { PromptTemplate } from '@langchain/core/prompts';
import { HuggingFaceInference } from '@langchain/community/llms/hf'; // Caminho mais prov√°vel para HuggingFaceInference
import dotenv from 'dotenv';
import readline from 'node:readline/promises';

dotenv.config();

// --- Template de Prompt para o LLM ---
// Este prompt ser√° usado para instruir o Mistral-7B-Instruct a gerar a resposta.
const LLM_PROMPT = PromptTemplate.fromTemplate(`[INST]
Voc√™ √© um assistente de IA focado em fornecer respostas precisas, concisas e √∫teis com base EXCLUSIVAMENTE no contexto fornecido.
N√£o utilize conhecimento externo.
Se a informa√ß√£o necess√°ria para responder √† pergunta n√£o estiver explicitamente no contexto, responda clara e diretamente que "N√£o foi poss√≠vel encontrar informa√ß√µes relevantes para a sua consulta na nossa base de dados."

Contexto:
{context}

Pergunta: {query}

Com base no contexto fornecido, responda √† pergunta de forma concisa e coerente.
[/INST]`);

// Configura√ß√£o do prompt para o "Painel de An√°lise" (mantido para debugging/visualiza√ß√£o)
const resultsPrompt = PromptTemplate.fromTemplate(`
 ## An√°lise de Documentos

 ### Contexto da Busca:
 - Termo pesquisado: "{query}"
 - N√∫mero de resultados (brutos): {rawCount}
 - Modelo de Embedding utilizado: {embeddingModel}

 ### Resultados Encontrados (ap√≥s filtro de relev√¢ncia):
 {results}

 ### Detalhes da An√°lise (para o desenvolvedor/monitoramento):
 1. Identifique os 3 principais insights
 2. Destaque termos-chave relevantes
 3. Avalie a consist√™ncia entre os documentos
`);

async function semanticSearch(query: string, k: number = 3) {
    let llmResponse = ""; // Vari√°vel para armazenar a resposta do LLM
    try {
        console.log(`\nüîç Iniciando busca por: "${query}"`);

        if (!redisVectorStore.redisClient.isOpen) {
            await redisVectorStore.redisClient.connect();
        }

        let results: [any, number][] = []; // Definindo o tipo para 'results'
        let embeddingModelUsed = process.env.EMBEDDINGS_MODEL || 'API Padr√£o';

        try {
            results = await redisVectorStore.similaritySearchWithScore(query, k);
        } catch (apiError) {
            console.warn('‚ö†Ô∏è Alternando para embeddings locais...');
            const { HuggingFaceTransformersEmbeddings } = await import('@langchain/community/embeddings/hf_transformers');
            embeddingModelUsed = 'Xenova/all-MiniLM-L6-v2 (local)';

            const localEmbeddings = new HuggingFaceTransformersEmbeddings({
                modelName: embeddingModelUsed
            });

            const localStore = new RedisVectorStore(localEmbeddings, {
                redisClient: redisVectorStore.redisClient,
                indexName: 'artigos-embeddings'
            });

            results = await localStore.similaritySearchWithScore(query, k);
        }

        // --- L√≥gica de Filtro de Relev√¢ncia ---
        // ATEN√á√ÉO: Ajuste este valor (0.7) conforme seus testes.
        // Scores mais ALTOS indicam mais relev√¢ncia para similaridade coseno.
        const RELEVANCE_THRESHOLD = 0.7;
        const filteredResults = results.filter(([, score]) => score >= RELEVANCE_THRESHOLD); // Alterado para `>=`

        let formattedRawResultsForDebug = ''; // Para o painel de an√°lise
        let contextForLLM = ''; // Conte√∫do que ser√° enviado ao LLM

        if (filteredResults.length === 0) {
            formattedRawResultsForDebug = 'Nenhum documento relevante encontrado ap√≥s filtragem.';
            llmResponse = "N√£o foi poss√≠vel encontrar informa√ß√µes relevantes para a sua consulta na nossa base de dados.";
        } else {
            // Formata√ß√£o dos resultados para o painel de an√°lise
            formattedRawResultsForDebug = filteredResults.map(([doc, score], index) => (
                `\n### Documento ${index + 1} (Score: ${score.toFixed(3)})\n` +
                `**T√≠tulo:** ${doc.metadata?.titulo || 'Sem t√≠tulo'}\n` +
                `**Trecho:** ${doc.pageContent.substring(0, 300)}${doc.pageContent.length > 300 ? '...' : ''}` // Maior trecho para o painel
            )).join('\n');

            // --- Prepara o Contexto para o LLM ---
            // √â importante passar apenas o conte√∫do relevante para o LLM
            contextForLLM = filteredResults.map(([doc, score], index) => (
                `Documento ${index + 1}:\n${doc.pageContent}`
            )).join('\n\n');

            // --- NOVO: Chamada ao LLM para Gerar a Resposta ---
            const chatModelName = process.env.CHAT_MODEL;
            // Garante que a API key esteja dispon√≠vel
            const huggingFaceApiKey = process.env.HUGGINGFACEHUB_API_TOKEN; 

            if (!chatModelName || !huggingFaceApiKey) {
                console.error("Vari√°veis de ambiente CHAT_MODEL ou HUGGINGFACEHUB_API_TOKEN n√£o configuradas. N√£o ser√° poss√≠vel gerar respostas.");
                llmResponse = "Erro: Modelo de chat ou chave API n√£o configurados para gerar respostas.";
            } else {
                const huggingFaceLLM = new HuggingFaceInference({
                    model: chatModelName,
                    apiKey: huggingFaceApiKey, // <--- AQUI EST√Å A SOLU√á√ÉO 2
                    maxTokens: 300, 
                    temperature: 0.3, 
                });

                try {
                    const llmPromptFormatted = await LLM_PROMPT.format({
                        query: query,
                        context: contextForLLM
                    });

                    const generatedText = await huggingFaceLLM.invoke(llmPromptFormatted, {
                        maxTokens: 300, 
                        temperature: 0.3, 
                    });

                    // Mistral-7B-Instruct geralmente retorna a resposta ap√≥s o [/INST]
                    const cleanGeneratedText = generatedText.split('[/INST]').pop()?.trim() || generatedText.trim();
                    llmResponse = cleanGeneratedText;

                } catch (llmError: any) {
                    console.error("‚ùå Erro ao chamar o LLM:", llmError.message || llmError);
                    llmResponse = "Houve um erro t√©cnico ao gerar a resposta. Por favor, tente novamente mais tarde.";
                }
            }
        }

        // --- Sa√≠da para o Usu√°rio (Resposta Gerada) ---
        console.log('\n‚úÖ Resposta Gerada:');
        console.log(llmResponse);

        // --- Painel de An√°lise (para debugging e detalhes internos) ---
        console.log('\n--- Painel de An√°lise (Detalhes Internos) ---');
        const analysisPrompt = await resultsPrompt.format({
            query,
            rawCount: results.length, // Mostra o n√∫mero de resultados antes do filtro
            embeddingModel: embeddingModelUsed,
            results: formattedRawResultsForDebug
        });
        console.log(analysisPrompt);

        // Detalhes t√©cnicos
        console.log('\nüîç Detalhes T√©cnicos Finais:');
        console.log(`Modelo LLM: ${process.env.CHAT_MODEL || 'N√£o configurado'}`);
        console.log(`Modelo Embedding: ${embeddingModelUsed}`);
        console.log(`Tempo: ${new Date().toLocaleTimeString()}`);

    } finally {
        if (redisVectorStore.redisClient.isOpen) {
            await redisVectorStore.redisClient.disconnect();
        }
    }
}

// Interface interativa melhorada
async function main() {
    const rl = readline.createInterface({
        input: process.stdin,
        output: process.stdout
    });

    try {
        console.log('\n=== Sistema de An√°lise Sem√¢ntica ===');
        console.log('üìå Modo:', process.env.HUGGINGFACEHUB_API_TOKEN ? 'API' : 'Local');

        while (true) {
            const query = await rl.question('\nüîé Digite sua consulta (ou "sair"): ');
            if (query.toLowerCase() === 'sair') break;

            const countInput = await rl.question('üìä N√∫mero de resultados para busca (3): ') || '3';
            await semanticSearch(query, parseInt(countInput));
        }
    } finally {
        rl.close();
        console.log('\nüõë Sess√£o encerrada');
    }
}

main().catch(err => {
    console.error('\n‚ùå Falha no sistema:', err);
    process.exit(1);
});