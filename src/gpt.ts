import { HuggingFaceInference } from '@langchain/community/llms/hf'
import { PromptTemplate } from '@langchain/core/prompts'
import { RetrievalQAChain } from 'langchain/chains'
import { redisVectorStore } from './redis-store'
import dotenv from 'dotenv'

dotenv.config()

// Modelos suportados em ordem de prioridade
const SUPPORTED_MODELS = [
  'mistralai/Mixtral-8x7B-Instruct-v0.1',  // Modelo alternativo
  'gpt2',                                  // Modelo fallback garantido
  'local'                                  // Ollama local
]

async function createLLM() {
  const apiKey = process.env.HUGGINGFACEHUB_API_TOKEN
  
  // Tentativa com Hugging Face
  if (apiKey?.startsWith('hf_')) {
    for (const model of SUPPORTED_MODELS) {
      try {
        if (model === 'local') break
        
        const llm = new HuggingFaceInference({
          apiKey,
          model,
          temperature: 0.3,
          maxRetries: 1,
          timeout: 15000
        })
        
        // Testa a conex√£o
        await llm.call('Teste de conex√£o')
        console.log(`\nüü¢ Modelo selecionado: ${model}`)
        return llm
      } catch (error) {
        console.warn(`‚ö†Ô∏è Modelo ${model} indispon√≠vel: ${error.message}`)
      }
    }
  }

  // Fallback para Ollama local
  try {
    const { ChatOllama } = await import('@langchain/community/chat_models/ollama')
    console.log('\nüîµ Usando modelo local (Ollama)')
    return new ChatOllama({
      baseUrl: 'http://localhost:11434',
      model: 'mistral',
      temperature: 0.3
    })
  } catch (error) {
    throw new Error('Nenhum modelo dispon√≠vel (nem Hugging Face, nem Ollama)')
  }
}

// Template otimizado
const prompt = PromptTemplate.fromTemplate(`
  Baseado nestes artigos:
  {context}

  Responda de forma t√©cnica:
  {question}

  Requisitos:
  - M√°ximo 3 par√°grafos
  - Formato markdown
  - Se n√£o souber: "Sem informa√ß√µes suficientes"`)

async function queryDocuments(question: string) {
  const llm = await createLLM()
  const chain = RetrievalQAChain.fromLLM(llm, redisVectorStore.asRetriever(3), {
    prompt,
    returnSourceDocuments: true
  })

  try {
    console.log('\nüîó Conectando ao Redis...')
    await redisVectorStore.redisClient.connect()

    console.log(`\nüîç Processando: "${question}"`)
    const response = await chain.invoke({ query: question })
    
    console.log('\nüí° Resposta:')
    console.log(response.text)
    
    if (response.sourceDocuments?.length > 0) {
      console.log('\nüìñ Fontes consultadas:')
      response.sourceDocuments.forEach((doc, i) => {
        console.log(`\nüìå Fonte ${i + 1}:`)
        console.log(`T√≠tulo: ${doc.metadata?.titulo || 'Sem t√≠tulo'}`)
        console.log(`Conte√∫do: ${doc.pageContent.substring(0, 120)}...`)
      })
    }

  } finally {
    if (redisVectorStore.redisClient?.isOpen) {
      await redisVectorStore.redisClient.disconnect()
    }
  }
}

// Execu√ß√£o com tratamento de erros completo
queryDocuments('Quais as vantagens do LangChain?')
  .catch(err => {
    console.error('\n‚ùå Falha cr√≠tica:', err.message)
    console.log('\nSolu√ß√µes poss√≠veis:')
    console.log('1. Obtenha uma chave em https://huggingface.co/settings/tokens')
    console.log('2. Instale Ollama local: curl -fsSL https://ollama.com/install.sh | sh')
    console.log('3. Use um modelo mais simples alterando SUPPORTED_MODELS')
    process.exit(1)
  })